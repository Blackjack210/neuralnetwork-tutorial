{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks Explained\n",
    "\n",
    "These types of networks are mainly used in Natural Language Processing (NLP) problems. One of its variants, LSTM, is one of the main models used today. \n",
    "\n",
    "<img src=\"images/recurrent.jpg\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "\n",
    "Feed-forward neural networks, take a fixed amount of input and produce a fixed amount of output. RNN takes data one at a time, in a sequence. At each step, RNN does calculations before producing an output. This output (called _hidden state_) is then combined with the following input in order to produce the next output.\n",
    "\n",
    "<img src=\"images/rnn.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "The cell tat is being used in the following steps is allways the same cell. \n",
    "\n",
    "Depending on what the goal is, one can extract different ammount of outputs. Whereas a classifier needs only one total output, text generation needs one output at every single time step.\n",
    "\n",
    "<img src=\"images/output1.jpg\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "<img src=\"images/output2.jpg\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "There are, naturally, more options than these two (see for example Sequence-To-Sequence translation).\n",
    "\n",
    "## Inner working\n",
    "\n",
    "Each cell must produce the following output (or _hidden_)\n",
    "\n",
    "$\\begin{equation}\n",
    "hidden_t = F(hidden_{t-1},input_t)\n",
    "\\end{equation}$\n",
    "\n",
    "It is usual to seed the first step with a matrix of zeros as the hidden state. The simplest model of RNN will multiply the states with weigth matrices, and passed through an activation function to introduce non-linearity (for example tanh):\n",
    "\n",
    "$\\begin{equation}\n",
    "hidden_t = tanh(weight_{hidden} * hidden_{t-1} + weight_{input}*input_t)\n",
    "\\end{equation}$\n",
    "\n",
    "and if we require an output at the end of each step, we also multiply the hidden layer:\n",
    "\n",
    "$\\begin{equation}\n",
    "output_t = weight_{output}*hidden_t\n",
    "\\end{equation}$\n",
    "\n",
    "We must then train our model, and for doing so we compare outputs with the correct answers through a loss function and hence its gradient.\n",
    "\n",
    "<img src=\"images/backprop.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "## Text processing\n",
    "\n",
    "There are many ways of encoding information. We will be working at character level, i.e. each character has its unique vector. The word \"Good\" has only 3 different characters and hecne a vocabulary size of 3.\n",
    "\n",
    "\n",
    "<img src=\"images/character.jpg\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "At the end of the code, we predict characters by bredicting vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
